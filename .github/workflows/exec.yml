name: Postgres Backup to S3-Compatible Storage

on:
  schedule:
    - cron: "0 4 * * *"
    - cron: "0 8 * * *"
    - cron: "0 12 * * *"
    - cron: "0 16 * * *"

  workflow_dispatch:

jobs:
  backup:
    runs-on: ubuntu-latest

    steps:
      - name: Install PostgreSQL client and AWS CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y curl jq python3-pip

          # Install AWS CLI via pip
          pip3 install awscli --upgrade --user
          echo "$HOME/.local/bin" >> $GITHUB_PATH

          # Add PostgreSQL APT repository for version 17
          sudo sh -c 'echo "deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
          wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
          sudo apt-get update

          # Install PostgreSQL 17 client
          sudo apt-get install -y postgresql-client-17

      - name: Create .dump backup
        run: |
          export PGPASSWORD=${{ secrets.PGPASSWORD }}
          TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
          FILE="backup_${PGDATABASE}_${TIMESTAMP}.dump"

          echo "=========================================="
          echo "üì¶ Creating PostgreSQL Backup"
          echo "=========================================="
          echo "Host: ${{ secrets.PGHOST }}"
          echo "Port: ${{ secrets.PGPORT }}"
          echo "Database: ${{ secrets.PGDATABASE }}"
          echo "Schema: ${{ secrets.PGSCHEMA }}"
          echo "Backup file: $FILE"
          echo "Timestamp: $TIMESTAMP"
          echo "=========================================="

          pg_dump \
            -h ${{ secrets.PGHOST }} \
            -p ${{ secrets.PGPORT }} \
            -U ${{ secrets.PGUSER }} \
            -d ${{ secrets.PGDATABASE }} \
            -n ${{ secrets.PGSCHEMA }} \
            --exclude-table=${{ secrets.PGSCHEMA }}.${{ secrets.PGMIGRATION_TABLE }} \
            --exclude-table=${{ secrets.PGSCHEMA }}.${{ secrets.PGMIGRATION_TABLE }}_id_seq \
            -F c -v -f $FILE

          echo ""
          echo "‚úÖ Backup created successfully!"
          echo "File size: $(du -h $FILE | cut -f1)"
          echo "BACKUP_FILE=$FILE" >> $GITHUB_ENV

      - name: Configure AWS CLI for S3 storage
        run: |
          ENDPOINT="${{ secrets.S3_ENDPOINT }}"

          echo "=========================================="
          echo "üîß Configuring AWS CLI for S3"
          echo "=========================================="

          if [ -n "$ENDPOINT" ]; then
            echo "Storage Type: S3-Compatible Storage"
            echo "S3 Endpoint: $ENDPOINT"
          else
            echo "Storage Type: AWS S3"
          fi

          echo "S3 Region: ${{ secrets.S3_REGION }}"
          echo ""

          # Configure AWS CLI
          aws configure set aws_access_key_id "${{ secrets.S3_ACCESS_KEY }}"
          aws configure set aws_secret_access_key "${{ secrets.S3_SECRET_KEY }}"
          aws configure set default.region "${{ secrets.S3_REGION }}"
          aws configure set default.s3.signature_version s3v4

          # Enable path-style addressing for S3-compatible storage
          if [ -n "$ENDPOINT" ]; then
            aws configure set default.s3.addressing_style path
            echo "Path-style addressing enabled for S3-compatible storage"
          fi

          echo "‚úÖ AWS CLI configured successfully!"

      - name: Upload file to S3 storage
        run: |
          FILE=${BACKUP_FILE}
          PREFIX="${{ secrets.S3_PREFIX }}"
          ENDPOINT="${{ secrets.S3_ENDPOINT }}"

          # Remove trailing slash from prefix if present
          PREFIX="${PREFIX%/}"

          # Construct the S3 path
          if [ -n "$PREFIX" ]; then
            S3_PATH="${PREFIX}/${FILE}"
          else
            S3_PATH="${FILE}"
          fi

          echo "=========================================="
          echo "‚òÅÔ∏è Uploading to S3 Storage"
          echo "=========================================="
          echo "File: $FILE"
          echo "Size: $(du -h $FILE | cut -f1)"
          echo "Destination bucket: ${{ secrets.S3_BUCKET }}"

          if [ -n "$ENDPOINT" ]; then
            echo "Storage Type: S3-Compatible Storage"
            echo "S3 Endpoint: $ENDPOINT"
          else
            echo "Storage Type: AWS S3"
          fi

          if [ -n "$PREFIX" ]; then
            echo "S3 Prefix (folder): $PREFIX"
          fi
          echo "Full S3 Path: $S3_PATH"
          echo ""
          echo "Uploading..."

          # Upload with or without endpoint URL
          if [ -n "$ENDPOINT" ]; then
            aws s3 cp "$FILE" \
              "s3://${{ secrets.S3_BUCKET }}/${S3_PATH}" \
              --endpoint-url "$ENDPOINT" \
              --region "${{ secrets.S3_REGION }}"
          else
            aws s3 cp "$FILE" \
              "s3://${{ secrets.S3_BUCKET }}/${S3_PATH}" \
              --region "${{ secrets.S3_REGION }}"
          fi

          if [ $? -eq 0 ]; then
            echo ""
            echo "‚úÖ Upload successful!"
            echo "S3_PATH=$S3_PATH" >> $GITHUB_ENV
          else
            echo ""
            echo "‚ùå Upload failed"
            exit 1
          fi

      - name: Cleanup old backups (older than retention days) via S3
        run: |
          RETENTION_DAYS="${{ secrets.RETENTION_DAYS }}"

          echo "=========================================="
          echo "üßπ Cleaning up old backups"
          echo "=========================================="

          # Skip deletion if retention is 0 or not provided
          if [ -z "$RETENTION_DAYS" ] || [ "$RETENTION_DAYS" -eq 0 ]; then
            echo "‚ÑπÔ∏è  Retention policy: Disabled (RETENTION_DAYS is not set or is 0)"
            echo "‚ÑπÔ∏è  Skipping cleanup - all backups will be kept"
            echo "=========================================="
            exit 0
          fi

          echo "Retention policy: Delete backups older than $RETENTION_DAYS days"
          echo ""

          BUCKET=${{ secrets.S3_BUCKET }}
          ENDPOINT="${{ secrets.S3_ENDPOINT }}"
          REGION=${{ secrets.S3_REGION }}
          PREFIX="${{ secrets.S3_PREFIX }}"

          # Remove trailing slash from prefix if present
          PREFIX="${PREFIX%/}"

          CUTOFF_DATE=$(date -d "$RETENTION_DAYS days ago" +%s)

          echo "Fetching list of backup files from bucket: $BUCKET"
          if [ -n "$PREFIX" ]; then
            echo "Searching in prefix (folder): $PREFIX"
          fi
          echo ""

          # List all objects in the bucket (or prefix) with timestamps
          # Handle both AWS S3 and S3-compatible storage
          if [ -n "$PREFIX" ]; then
            if [ -n "$ENDPOINT" ]; then
              aws s3api list-objects-v2 \
                --bucket "$BUCKET" \
                --prefix "${PREFIX}/" \
                --endpoint-url "$ENDPOINT" \
                --region "$REGION" \
                --query 'Contents[?contains(Key, `backup_`)].{Key:Key,LastModified:LastModified}' \
                --output json > /tmp/files.json
            else
              aws s3api list-objects-v2 \
                --bucket "$BUCKET" \
                --prefix "${PREFIX}/" \
                --region "$REGION" \
                --query 'Contents[?contains(Key, `backup_`)].{Key:Key,LastModified:LastModified}' \
                --output json > /tmp/files.json
            fi
          else
            if [ -n "$ENDPOINT" ]; then
              aws s3api list-objects-v2 \
                --bucket "$BUCKET" \
                --endpoint-url "$ENDPOINT" \
                --region "$REGION" \
                --query 'Contents[?contains(Key, `backup_`)].{Key:Key,LastModified:LastModified}' \
                --output json > /tmp/files.json
            else
              aws s3api list-objects-v2 \
                --bucket "$BUCKET" \
                --region "$REGION" \
                --query 'Contents[?contains(Key, `backup_`)].{Key:Key,LastModified:LastModified}' \
                --output json > /tmp/files.json
            fi
          fi

          if [ ! -s /tmp/files.json ] || [ "$(cat /tmp/files.json)" == "null" ]; then
            echo "‚ÑπÔ∏è  No backup files found in bucket"
            exit 0
          fi

          TOTAL_FILES=$(jq 'length' /tmp/files.json)
          echo "Total backup files in bucket: $TOTAL_FILES"
          echo ""

          DELETED_COUNT=0
          KEPT_COUNT=0

          # Process each file using a while loop with process substitution to preserve variables
          while IFS= read -r item; do
            FILE_KEY=$(echo "$item" | jq -r '.Key')
            LAST_MODIFIED=$(echo "$item" | jq -r '.LastModified')
            
            # Convert LastModified to Unix timestamp
            FILE_TS=$(date -d "$LAST_MODIFIED" +%s)
            AGE_DAYS=$(( ($(date +%s) - FILE_TS) / 86400 ))
            
            if [ $FILE_TS -lt $CUTOFF_DATE ]; then
              echo "üóëÔ∏è  Deleting: $FILE_KEY (Age: $AGE_DAYS days, Modified: $LAST_MODIFIED)"
              
              # Delete with or without endpoint URL
              if [ -n "$ENDPOINT" ]; then
                aws s3 rm "s3://$BUCKET/$FILE_KEY" --endpoint-url "$ENDPOINT" --region "$REGION"
              else
                aws s3 rm "s3://$BUCKET/$FILE_KEY" --region "$REGION"
              fi
              
              DELETED_COUNT=$((DELETED_COUNT + 1))
            else
              echo "‚úÖ Keeping: $FILE_KEY (Age: $AGE_DAYS days, Modified: $LAST_MODIFIED)"
              KEPT_COUNT=$((KEPT_COUNT + 1))
            fi
          done < <(jq -c '.[]' /tmp/files.json)

          echo ""
          echo "=========================================="
          echo "Cleanup Summary:"
          echo "  - Retention period: $RETENTION_DAYS days"
          echo "  - Files deleted: $DELETED_COUNT"
          echo "  - Files kept: $KEPT_COUNT"
          echo "=========================================="

      - name: Done
        run: |
          echo ""
          echo "=========================================="
          echo "‚úÖ BACKUP WORKFLOW COMPLETED SUCCESSFULLY!"
          echo "=========================================="
          echo "Backup file: ${BACKUP_FILE}"
          echo "Upload location: ${{ secrets.S3_BUCKET }}/${S3_PATH}"
          echo "Timestamp: $(date)"
          echo "=========================================="
